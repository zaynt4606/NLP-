## **分布式图数据挖掘**

#### <u>隐狄利克雷分布LDA</u>

[狄利克雷分布(Dirichlet Distribution) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/78743630#:~:text=什么是狄利克雷分布？ 狄利克雷分布是一种“分布的分布” (a distribution on probability distribution) ，由两个参数,parameter)

[深入机器学习系列11－隐式狄利克雷分布 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/29257655#:~:text=LDA是一种概率主题模型：隐式狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。 LDA是2003年提出的一种 主题模型,，它可以将文档集中每篇文档的主题以概率分布的形式给出。 通过分析一些文档，我们可以抽取出它们的主题（分布），根据主题（分布）进行主题聚类或文本分类。 同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。 一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。 举一个简单的例子，比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习的方式，获取每个主题Topic对应的词语，如下图所示：)

[隐含狄利克雷分布（LDA） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/29932017)

Dirichlet分布，是beta分布在高维度上的推广。Dirichlet分布的密度函数形式跟beta分布的密度函数类似：

![img](https://pic1.zhimg.com/80/v2-e36eb7c517c5d2bfa67597e2bcd38a88_720w.png)

![img](https://pic4.zhimg.com/80/v2-ce2b1c7ce8fc296308277c0d1a72e6b3_720w.png)

Beta分布是二项式分布的共轭先验概率分布。那么Dirichlet分布呢？Dirichlet分布是多项式分布的共轭先验概率分布。

LDA主题模型

LDA是一种概率主题模型：隐式狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。LDA是2003年提出的一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出。 通过分析一些文档，我们可以抽取出它们的主题（分布），根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

区分pLSA主题分布

LDA是pLSA的贝叶斯版本，因为LDA被贝叶斯化了，所以才会加两个先验参数

在pLSA中，给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示:

<img src="C:\Users\HP\Desktop\read\面试\NLP面经\pic\plsa.jpg" alt="plsa" style="zoom:50%;" />

在LDA中，主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的。LDA为提供了两个Dirichlet先验参数，Dirichlet先验为某篇文档随机抽取出主题分布和词分布。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6，而主题分布到底是哪个取值集合我们不确定，但其先验分布是dirichlet分布，所以可以从无穷多个主题分布中按照dirichlet先验随机抽取出某个主题分布出来。如下图所示：

<img src="C:\Users\HP\Desktop\read\面试\NLP面经\pic\lda.jpg" alt="lda" style="zoom:50%;" />

LDA在pLSA的基础上给两参数$P(z_k|d_i)$ 和$P(w_i|z_k)$  加了两个先验分布的参数。这两个分布都是Dirichlet分布。 下面是LDA的图模型结构：

![lda图模型](C:\Users\HP\Desktop\read\面试\NLP面经\pic\lda图模型.png)

LDA参数估计：spark中提供了两种方法估计参数：变分EM算法、在线学习算法。



#### <u>Gibbs吉布斯采样</u>

MCMC采样(随机采样方法)_Markov Chain Monte Carlo马尔科夫链蒙特卡罗

[MCMC(一)蒙特卡罗方法 ](https://www.cnblogs.com/pinard/p/6625739.html#:~:text=MCMC (四)Gibbs采样. 作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习%2C深度学习以及自然语言处理等领域都有广泛的应用，是很多复杂算法求解的基础。. 比如我们前面讲到的,分解机 (Factorization Machines)推荐算法 ，还有前面讲到的 受限玻尔兹曼机（RBM）原理总结 ，都用到了MCMC来做一些复杂运算的近似求解。. 下面我们就对MCMC的原理 )

[MCMC(二)马尔科夫链 ](https://www.cnblogs.com/pinard/p/6632399.html)

[MCMC(三)MCMC采样和M-H采样](https://www.cnblogs.com/pinard/p/6638955.html)

　M-H采样完整解决了使用蒙特卡罗方法需要的任意概率分布样本集的问题，因此在实际生产环境得到了广泛的应用。

　　　　但是在大数据时代，M-H采样面临着两大难题：

　　　　1） 我们的数据特征非常的多，M-H采样由于接受率计算式的存在，在高维时需要大量计算时间，算法效率很低。同时$\alpha(i,j)$一般小于1，可能辛苦计算出来却被拒绝了。能否不拒绝转移？

　　　　2） 由于特征维度大，目标的各特征维度联合分布难求，特征之间的条件概率分布好求。这时候我们能不能只有各维度之间条件概率分布的情况下方便的采样呢？

　　　　Gibbs采样解决了上面两个问题。

[MCMC(四)Gibbs采样 ](https://www.cnblogs.com/pinard/p/6645766.html)

　由于Gibbs采样在高维特征时的优势，目前我们通常意义上的MCMC采样都是用的Gibbs采样。当然Gibbs采样是从M-H采样的基础上的进化而来的，同时Gibbs采样要求数据至少有两个维度，一维概率分布的采样是没法用Gibbs采样的,这时M-H采样仍然成立。



 

## **GBDT**

### GBDT = Gradient boosting + CART 决策树

GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）是监督学习中一种常见的集成树模型，可以用于处理分类和回归问题。

1）回归树：GBDT 中的树都是回归树

2）梯度迭代：GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0，这样后面的树就能专注于那些被分错的样本

3）缩减：不直接用残差修复误差，而是只修复一点点，把大步切成小步。

4）梯度提升：一种基于函数梯度信息的Boosting方法，与梯度下降有异曲同工之妙。在每一轮迭代时，我们生成一个基学习器，基学习器的拟合目标是当前模型Loss的负梯度。当训练完成后，我们将该基学习器加入至模型。重复上述，继续训练基学习器，直至迭代次数达到目标。

**优点**

1. 可以自动进行特征组合，拟合非线性数据；
2. 可以灵活处理各种类型的数据。

**缺点**

1. 对异常点敏感。

与Adaboost的对比

**相同：**

1. 都是 Boosting 家族成员，使用弱分类器；
2. 都使用前向分布算法；

**不同：**

1. **迭代思路不同**：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；
2. **损失函数不同**：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；

### 6、XgBoost

XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包

**优点**

1. **精度更高：**GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
2. **灵活性更强：**GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
3. **正则化：**XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
4. **Shrinkage（缩减）：**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；
5. **列抽样：**XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. **缺失值处理：**XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；
7. **可以并行化操作：**块结构可以很好的支持并行计算。

**缺点**

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。



















