## RNN与LSTM

1、梯度爆炸与梯度消失



2、梯度消失与梯度爆炸的解决方法

梯度爆炸：设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸

梯度消失：**预训练加微调、使用relu、残差网络、**

​        **batchnorm**：



3、LSTM有输入门、输出门、遗忘门

遗忘门：用于决定应丢弃或保留的信息

输入门：用于对输入信息进行过滤，选择性的抛弃一些信息，和候选细胞状态共同决定当前细胞状态的更新，决定长期记忆

输出门：用sigmoid层来确定细胞状态的哪个部分将输出出去，ht决定短期记忆

错，引入松弛变量 εi≥0，公式变为：



那么这个松弛变量怎么计算呢，最开始试图用0，1损失去计算，但0，1损失函数并不连续，求最值时求导的时候不好求，所以引入合页损失（hinge loss）：



# CNN



1、1*1卷积的主要作用有以下几点：

1）**降维（ dimension reductionality ）**。比如，一张500 * 500且厚度depth为100 的图片在20个filter上做1*1的卷积，那么结果的大小为500*500*20。

2）**加入非线性**。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；

 2、各层作用

- 卷积层

  1. 提取图像的特征，并且卷积核的权重是可以学习的，由此可以猜测，在高层神经网络中，卷积操作能突破传统滤波器的限制，根据目标函数提取出想要的特征。
  2. “局部感知，参数共享”的特点大大降低了网络参数，保证了网络的稀疏性，防止过拟合，之所以可以“参数共享”，是因为样本存在局部相关的特性。

- 激活层

  对卷积层的输出结果做一次非线性映射。

  如果不用激励函数（其实就相当于激励函数是f(x)=x），这种情况下，每一层的输出都是上一层输入的线性函数。容易得出，无论有多少神经网络层，输出都是输入的线性组合，与没有隐层的效果是一样的，这就是最原始的感知机了。

- 池化层（采样层、pooling）

  pooling池化的作用则体现在降采样：保留显著特征、降低特征维度，增大kernel的感受野。另外一点值得注意：pooling也可以提供一些旋转不变性。

  池化层可对提取到的特征信息进行降维，一方面使特征图变小，简化网络计算复杂度并在一定程度上避免过拟合的出现；一方面进行特征压缩，提取主要特征。

  池化层的padding：padding可以认为是扩充图片， 在图片周围补充一些像素点，把这些像素点初始化为0。