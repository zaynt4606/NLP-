- SVM（原理）
- LR（为啥用sigmoid函数，交叉熵推导，MAE和MLP，反向传播，归一化，正则化）
- 降维算法（SVD和PCA）
- K-Means（手撕代码实现）
- [决策树](https://www.zhihu.com/search?q=决策树&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2304371291})（各种生成和剪枝方法）
- 集成学习（随机森林、XGBoost、AdaBoost、GBDT）
- EM算法（原理）
- 过拟合（[正则化](https://www.zhihu.com/search?q=正则化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2304371291})、增加训练数据、数据增强、标签平滑、BatchNorm、Early-Stop、交叉验证、Dropout、Pre-trained、引入先验知识）
- 方差偏差分解（解释什么是方差什么是偏差，公式推导）
- 正则化（L1和L2的会有啥现象、解释原因、分别代表什么先验，bias要不要正则）
- 初始化（不同网络初始化有啥区别，神经网络隐层可以全部初始化为0吗）
- 激活函数（优缺点，sigmoid、tanh、relu、gelu）
- 损失函数（用过哪些损失函数，为啥分类不用MSE）
- [信息论](https://www.zhihu.com/search?q=信息论&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2304371291})（信息熵、条件熵、联合熵、相对熵、互信息的概念，交叉熵和KL散度区别）
- [归一化](https://www.zhihu.com/search?q=归一化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2304371291})（为什么要做归一化，各种归一化的区别和优缺点，NLP为啥不用BatchNorm）
- 梯度消失（残差、门控、sigmoid换relu、归一化）
- 梯度爆炸（截断）
- 优化器（原理和演进过程，SGD、AdaGrad、RMSprop、AdaDelta、Adam、AdamW）
- 显存爆炸（重计算、梯度累加、混合精度训练、Adam换成SGD、多用inplace）
- 学习率（衰减、warmup、自适应、平时自己使用的时候对lr有什么调整心得吗）
- 样本不均衡（降/过采样和带权重的loss）
- 数据预处理（离散特征和连续特征）
- 评价指标（Acc、Precision、Recall、F1、ROC、AUC、代码实现AUC）
- 神经网络（优缺点、演进和公式推导，lstm、cnn、transformer）
- OOV咋办

# 机器学习类

## batch_size与学习率

以随机梯度下降为例



学习率（lr）直观可以看出lr越大，权重更新的跨度越大，模型参数调整变化越快



## 评估指标

1、ROC与AUC

**ROC**要计算FPR和TPR，曲线上的每个点代表不同阈值时的FPR和TPR

在正负样本数量不均衡的时候，比如负样本的数量增加到原来的10倍，那TPR不受影响，FPR的各项也是成比例的增加，并不会有太大的变化。因此，在样本不均衡的情况下，同样ROC曲线仍然能较好地评价分类器的性能，这是ROC的一个优良特性，也是为什么一般ROC曲线使用更多的原因。

**AUC**  [https://www.zhihu.com/search?type=content&q=正负样本比例变化auc](https://www.zhihu.com/search?type=content&q=%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%AF%94%E4%BE%8B%E5%8F%98%E5%8C%96auc)

AUC即ROC曲线下的面积，AUC越大，说明分类器越可能把正样本排在前面，衡量的是一种排序的性能。

AUC的概率意义：就是从样本中任意取一个正例和一个负例，正例得分大于负例得分的概率。



AUC的优点：

- AUC衡量的是一种排序能力，因此特别适合排序类业务；
- AUC对正负样本均衡并不敏感，在样本不均衡的情况下，也可以做出合理的评估。
- 其他指标比如precision，recall，F1，根据区分正负样本阈值的变化会有不同的结果，而AUC不需要手动设定阈值，是一种整体上的衡量方法。

AUC的缺点：

- 忽略了预测的概率值和模型的拟合优度；
- AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标；
- 它没有给出模型误差的空间分布信息，AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序，这样我们也无法衡量样本对于好坏程度的刻画能力；

## 常见激活函数：

- sigmoid

  

  第一点，最明显的就是饱和性，从上图也不难看出其两侧导数逐渐趋近于0， 即limf'(x)=0。具体来说，在反向传播的过程中，sigmoid的梯度会包含了一个f'(x) 因子（sigmoid关于输入的导数），因此一旦输入落入两端的饱和区，f'(x)就会变得接近于0，导致反向传播的梯度也变得非常小，此时网络参数可能甚至得不到更新，难以有效训练，这种现象称为梯度消失。一般来说，sigmoid网络在5层之内就会产生梯度消失现象。

  **sigmoid与softmax:**

  如果模型输出为非互斥类别，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。

  如果模型输出为互斥类别，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。

  Sigmoid函数可以用来解决多标签问题，Softmax函数用来解决单标签问题。

  对于某个分类场景，当Softmax函数能用时，Sigmoid函数一定可以用。

- tanh

  $tanh(x) = \frac{1-{e}^{-2x}}{1+e^{-2x}}$

  

- relu

  $ReLu(x)=max(0,x)$

  

  优点：

  - `解决了梯度消失、爆炸的问题`
  - `计算方便，计算速度快`
  - `加速了网络的训练`

  缺点：

  - `由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）`
  - `输出不是以0为中心的`

## softmax与signmoid

**如何用softmax做多分类和多标签分类**

现假设，神经网络模型最后的输出是这样一个向量1, 就是神经网络最终的全连接的输出。这里假设总共有`4`个分类

首先用`softmax`将`logits`转换成一个概率分布，然后取概率值最大的作为样本的分类这样看似乎，`tf.argmax(logits)`同样可以取得最大的值，也能得到正确的样本分类，这样的话`softmax`似乎作用不大 那么`softmax`的主要作用其实是在计算交叉熵上，首先样本集中`y`是一个`one-hot`向量，如果直接将模型输出`logits`和`y`来计算交叉熵，因为`logits=[1,2,3,4]`，计算出来的交叉熵肯定很大，这种计算方式不对，而应该将`logits`转换成一个概率分布后再来计算,就是用`tf.softmax(logits)`和`y`来计算交叉熵，当然我们也可以直接用`tensorflow`提供的方法`sofmax_cross_entropy_with_logits`来计算这个方法传入的参数可以直接是`logits`，因为这个根据方法的名字可以看到，方法内部会将参数用`softmax`进行处理现在我们取的概率分布中最大的作为最终的分类结果，这是多分类我们也可以取概率的`top`几个，作为最终的多个标签，或者设置一个阈值，并取大于概率阈值的。这就用`softmax`实现了多标签分类

**sigmoid激活函数应用于多标签分类**

`sigmoid`一般不用来做多类分类，而是用来做二分类，它是将一个标量数字转换到`[0,1]`之间，如果大于一个概率阈值`(一般是0.5)`，则认为属于某个类别，否则不属于某个类别。这一属性使得其适合应用于多标签分类之中，在多标签分类中，大多使用`binary_crossentropy`损失函数。它是将一个标量数字转换到`[0,1]`之间，如果大于一个概率阈值`(一般是0.5)`，则认为属于某个类别。本质上其实就是针对`logits`中每个分类计算的结果分别作用一个`sigmoid`分类器，分别判定样本是否属于某个类别同样假设，神经网络模型最后的输出是这样一个向量`logits=[1,2,3,4,5,6,7,8,9,10]`, 就是神经网络最终的全连接的输出。这里假设总共有`10`个分类。通过：

`tf**.**sigmoid(logits)`

`sigmoid`应该会将`logits`中每个数字都变成`[0,1]`之间的概率值，假设结果为`[0.01, 0.05, 0.4, 0.6, 0.3, 0.1, 0.5, 0.4, 0.06, 0.8]`, 然后设置一个概率阈值，比如`0.3`，如果概率值大于`0.3`，则判定类别符合，那么该输入样本则会被判定为`类别3`、`类别4`、`类别5`、`类别7`及`类别8`。即一个样本具有多个标签。 **在这里强调一点：将`sigmoid`激活函数应用于多标签分类时，其损失函数应设置为`binary_crossentropy`。**

## L1正则化和L2正则化分别适用于什么样的场景

如果需要稀疏性就用l1，因为l1的梯度是1或-1，所以每次更新都稳步向0趋近。

一般多用l2因为计算方便“求导置零解方程”，l2只有最好的一条预测线而l1可能有多个最优解。

l1鲁棒性更强对异常值不敏感

## 归一化和标准化

**归一化**：对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。

好处：

1 提高迭代求解的收敛速度，不归一化梯度可能会震荡

2 提高迭代求解的精度



**标准化**：对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。

好处

1 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上

2 不改变原始数据的分布



## 逻辑回归

1、**逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，利用梯度下降求解参数，来达到将数据二分类的目的**。

2、逻辑回归的第二个假设是：样本为正的概率  $p=\frac{1}{1+e^{-\theta ^{T}x}}$ ，逻辑回归即为h(x;θ)=p

3、逻辑回归的损失函数就是h(x;θ)的极大似然函数



**极大似然的核心思想是如果现有样本可以代表总体，那么极大似然估计就是找到一组参数使得出现现有样本的可能性最大。**

4、逻辑回归的求解方法：



5、逻辑回归的sigmoid的输出可以表示概率么？

sigmoid 本身不能赋予输出概率意义，而是 cross entropy 这样有概率意义的损失函数，才让输出可以解释成概率的



6、逻辑回归为什么用sigmoid？

由广义线性模型模型做二分类，变量服从伯努利分布，最后推到可以得到sigmoid的形式，得出结论，当因变量服从伯努利分布时，广义线性模型就为逻辑回归。

## SVM

1、SVM本质是在求支持向量到超平面的几何间隔的最大值

2、svm适合解决小样本、非线性、高纬度的问题

3、核函数：样本集在低维空间线性不可分时，核函数将原始数据映射到高维空间，或者增加数据维度，使得样本线性可分。

常用核函数：
线性核：不增加数据维度，而是余弦计算内积，提高速度
多项式核：增加多项式特征，提升数据维度，并计算内积
高斯核（默认BRF）：将样本映射到无限维空间，使原来不可分的样本线性可分

4、svm的目标函数（硬间隔）

有两个目标：第一个是使间隔最大化，第二个是使样本正确分类，由此推出目标函数：





这是一个有约束条件的最优化问题，用拉格朗日函数来解决



5、svm软间隔

不管直接在原特征空间，还是在映射的高维空间，我们都假设样本是线性可分的。虽然理论上我们总能找到一个高维映射使数据线性可分，但在实际任务中，寻找一个合适的核函数核很困难。此外，由于数据通常有噪声存在，一味追求数据线性可分可能会使模型陷入过拟合，因此，我们放宽对样本的要求，允许少量样本分类错误。这样的想法就意味着对目标函数的改变，之前推导的目标函数里不允许任何错误，并且让间隔最大，现在给之前的目标函数加上一个误差，就相当于允许原先的目标出错，引入松弛变量 εi≥0，公式变为：



那么这个松弛变量怎么计算呢，最开始试图用0，1损失去计算，但0，1损失函数并不连续，求最值时求导的时候不好求，所以引入合页损失（hinge loss）：





## 对比SVM和LR

- LR与SVM的相同点：
  - 都是有监督的分类算法；
  - 如果不考虑核函数，LR和SVM都是线性分类算法。
  - 它们的分类决策面都是线性的。
  - LR和SVM都是判别式模型。
- LR与SVM的不同点：
  - 本质上是loss函数不同，或者说分类的原理理不不同。
  - SVM是结构风险最小化（经验+正则），LR则是经验风险最小化。
  - SVM只考虑分界面附近的少数点，而LR则考虑所有点。
  - 在解决非线性问题时，SVM可采用核函数的机制，而LR通常不采用核函数的方法。
  - SVM计算复杂，但效果比LR好，适合小数据集；LR计算简单，适合大数据集，可以在线训练。

# 损失函数

1、交叉熵损失函数



2、均方误差损失mean squard error



# 优化器

1、梯度下降

- 批量梯度下降

  在每次更新时用所有样本，要留意，在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整 θ .其计算得到的是一个标准梯度，对于最优化问题，凸问题，也肯定可以达到一个全局最优。

  因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦。但是很多时候，样本很多，更新一次要很久

  

- 随机梯度下降

  在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，**对于最优化问题，凸问题，**虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。但是相比于批量梯度，这样的方法更快，更快收敛，虽然不是全局最优，但很多时候是我们可以接受的

- 小批量梯度下降

  在每次更新时用b个样本,其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是我1个指不定不太准，那我用个30个50个样本那比随机的要准不少了吧，而且批量的话还是非常可以反映样本的一个分布情况的。在深度学习中，这种方法用的是最多的，因为这个方法收敛也不会很慢，收敛的局部最优也是更多的可以接受！

2、引入动量

SGD方法中的高方差振荡使得网络很难稳定收敛，所以有研究者提出了一种称为动量（Momentum）的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练。换句话说，这种新方法将上个步骤中更新向量的分量’γ’添加到当前更新向量。

在参数更新过程中，其原理类似：

1) 使网络能更优和更稳定的收敛；

2) 减少振荡过程。

当其梯度指向实际移动方向时，动量项γ增大；当梯度与实际移动方向相反时，γ减小。这种方式意味着动量项只对相关样本进行参数更新，减少了不必要的参数更新，从而得到更快且稳定的收敛，也减少了振荡过程。

3、Adagrad

Adagrad方法是通过参数来调整合适的学习率η，对稀疏参数进行大幅更新和对频繁参数进行小幅更新。因此，Adagrad方法非常适合处理稀疏数据。

在时间步长中，Adagrad方法基于每个参数计算的过往梯度，为不同参数θ设置不同的学习率。

Adagrad方法的主要好处是，不需要手工来调整学习率。大多数参数使用了默认值0.01，且保持不变。

Adagrad方法的主要缺点是，学习率η总是在降低和衰减。

4、AdaDelta

解决了Adagrad学习率总是在降低衰减的缺点

AdaDelta方法的另一个优点是，已经不需要设置一个默认的学习率。

5、Adam

在之前的方法中计算了每个参数的对应学习率，但是为什么不计算每个参数的对应动量变化并独立存储呢？这就是Adam算法提出的改良点。

Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的**自适应学习率。同时也结合了动量**

在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。

# 决策树

### 1、决策树的生成算法

信息熵越大信息的纯度越低

数据的信息熵：  $-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}log_{2}\frac{\left | C_{k} \right |}{\left | D \right |}$    Ck表示集合D中属于第k类样本的样本子集

信息增益=信息熵-条件熵

1）ID3：根据信息增益分类

步骤：

1. 初始化特征集合和数据集合；
2. 计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
3. 更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。

缺点：

- ID3 没有剪枝策略，容易过拟合；
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
- 只能用于处理离散分布的特征；
- 没有考虑缺失值。

2）C4.5：根据信息增益率划分

剪枝策略：

预剪枝

在节点划分前来确定是否继续增长，及早停止增长的主要方法有：

- 节点内数据样本低于某一阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。

预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。

后剪枝

在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。

C4.5 采用的**悲观剪枝方法**，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。

缺点：

- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。

3）CART：根据基尼指数分类，越小越好



pk为Ck表示集合D中属于第k类样本的样本子集的概率

CART 在 C4.5 的基础上进行了很多提升。

- C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；
- C4.5 只能分类，CART 既可以分类也可以回归；
- CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；
- CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；
- CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。

CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。

**对比：**

- **划分标准的差异：**ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
- **使用场景的差异：**ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
- **样本数据的差异：**ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
- **样本特征的差异：**ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
- **剪枝策略的差异：**ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

### 2、集成学习

1）bagging(boostrap aggregating)

每个基学习器都会对训练集进行有放回抽样得到子训练集，每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。



2)boosting 

基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法



3)Stacking

并通过训练一个元模型来组合它们，然后基于这些弱模型返回的多个预测结果形成新的训练集，通过新的训练集去训练组合模型得到新模型去预测。其中用了k-折交叉训练

假设我们想要拟合由 L 个弱学习器组成的 stacking 集成模型。我们必须遵循以下步骤：

- 将训练数据分为两组
- 选择 L 个弱学习器，用它们拟合第一组数据
- 使 L 个学习器中的每个学习器对第二组数据中的观测数据进行预测
- 在第二组数据上拟合元模型，使用弱学习器做出的预测作为输入

在前面的步骤中，我们将数据集一分为二，因为对用于训练弱学习器的数据的预测与元模型的训练不相关。因此，将数据集分成两部分的一个明显缺点是，我们只有一半的数据用于训练基础模型，另一半数据用于训练元模型。



**为什么集成学习会好于单个学习器呢？**

1. 训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；
2. 假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；
3. 可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。

**Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）。**

### 3、随机森林 = Bagging+CART决策树

步骤：

1. 随机选择样本（放回抽样）；
2. 随机选择特征；
3. 构建决策树；
4. 随机森林投票（平均）。

随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。

**优点**

1. 在数据集上表现良好，相对于其他算法有较大的优势
2. 易于并行化，在大数据集上有很大的优势；
3. 能够处理高维度数据，不用做特征选择。

### 4、Adaboost

前一个基本分类器**分错的样本会得到加强**，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

Adaboost 迭代算法有三步：

1. 初始化训练样本的权值分布，每个样本具有相同权重；
2. 训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；
3. 将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。

**优点**

1. 分类精度高；
2. 可以用各种回归分类模型来构建弱学习器，非常灵活；
3. 不容易发生过拟合。

**缺点**

1. 对异常点敏感，异常点会获得较高权重

### 5、GBDT = Gradient boosting + CART 决策树

GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）

1）回归树：GBDT 中的树都是回归树

2）梯度迭代：GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0，这样后面的树就能专注于那些被分错的样本

3）缩减：不直接用残差修复误差，而是只修复一点点，把大步切成小步。

**优点**

1. 可以自动进行特征组合，拟合非线性数据；
2. 可以灵活处理各种类型的数据。

**缺点**

1. 对异常点敏感。

与Adaboost的对比

**相同：**

1. 都是 Boosting 家族成员，使用弱分类器；
2. 都使用前向分布算法；

**不同：**

1. **迭代思路不同**：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；
2. **损失函数不同**：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；

### 6、XgBoost

XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包

**优点**

1. **精度更高：**GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
2. **灵活性更强：**GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
3. **正则化：**XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
4. **Shrinkage（缩减）：**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；
5. **列抽样：**XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. **缺失值处理：**XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；
7. **可以并行化操作：**块结构可以很好的支持并行计算。

**缺点**

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。