**词向量模型**（NNLM(neural-net language model)、word2vec和glove）

<u>0、(静态)word2vec - glove -(动态) ELMo - GPT - BERT - others的演进，每个模型分别解决什么问题</u>

**word2vec**：

目标是学习词向量并且支持线性的语义运算比如“皇帝-皇后=男-女”，另外word2vec的一个精髓是把语言模型的那一套softmax加速方法也给顺便优化了，用一个看似开脑洞的“负采样”方法来代替传统的层级softmax和NCE做法。

**GloVe**：

目标是进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。基于语料库构建共现矩阵，然后基于共现矩阵和glove模型学习词向量。

Cbow/Skip-Gram 是一个local context window的方法，比如使用NS(negative samoling负采样)来训练，缺乏了整体的词和词的关系，负样本采用sample的方式会缺失词的关系信息。另外，直接训练Skip-Gram类型的算法，很容易使得高曝光词汇得到过多的权重

Global Vector融合了矩阵分解Latent Semantic Analysis (LSA)的全局统计信息和local context window优势。融入全局的先验统计信息，可以加快模型的训练速度，又可以控制词的相对权重。

**ELMo**：Embeddings from Language Model

目标是学习到上下文相关的、更强大的词向量，目的依然是为下游任务提供一个扎实的根基；深度：单词表示组合了深度预训练神经网络的所有层；基于字符：ELMo表示纯粹基于字符，然后经过CharCNN之后再作为词的表示，解决了OOV问题，而且输入的词表也很小；

ELMo在模型层上就是一个stacked bi-lstm（严格来说是训练了两个单向的stacked lstm），所以当然有不错的encoding能力。拼接方式双向融合，特征融合能力偏弱，并且抽取特征能力弱于Transformer

通过实验间接说明了在多层的RNN中，不同层学到的特征其实是有差异的，因此ELMo提出在预训练完成并迁移到下游NLP任务中时，要为原始词向量层和每一层RNN的隐层都设置一个可训练参数，这些参数通过softmax层归一化后乘到其相应的层上并求和便起到了weighting的作用，然后对“加权和”得到的词向量再通过一个参数来进行词向量整体的scaling以更好的适应下游任务。

**GPT**：

GPT采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。GPT和ELMO是类似的都是两阶段模型，但是GPT特征抽取是Transformer，elmo是基于LSTM，并且GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，只使用了上文，这对于完成阅读理解等任务存在缺陷。

第一阶段：Embedding——>Transformer——>Text Production

第二阶段：fine-tuning把任务的网络结构改造成和GPT的网络结构是一样的，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到任务里了，再使用任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决任务的问题。

**BERT**:

BERT和GPT采用完全相同的两阶段模型，先是语言模型预训练，其次是使用fine-tuning模式解决下游任务。

相较于OpenAI GPT模型而言，其为**双向**Transformer，masked双向语言模型，语言模型的数据规模要比GPT大

**其他：XLNet、NEZHA**



<u>1、相对于NNLM，word2vec的改进有哪些</u>

neural net language model使用神经网络构建N元模型，基于相似词语解决数据稀疏的问题，将模型的第一层特征映射矩阵当做词的分布式表示。但是计算复杂度过大，只对词的左侧文本进行建模，得到的词向量不能充分表征语义。主要目的是得到语言模型，过程中求解出词向量。word2vec主要目的是计算词向量。

对比nnlm，word2vec优化网络结构，参数量大量减少，与上下文所取词个数无关避免了n-gram中随着阶数n增大计算复杂度急剧上升的问题。优化softmax归一化，分别对应负采样和hierarchical softmax（用哈夫曼树实现分层softmax）



<u>2、**负采样**的具体实现方法</u>

负采样样本数量选择

一个单词被选作 negative sample 的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words

> 论文认为对于小规模数据集 5-20 个 negative words比较合适，对于大规模数据集选择 2-5个 negative words.

为什么小规模的数据集要多采样一些？数据集规模越大，单词就越多，A没有挨着的单词都算负采样，就比如有1000个词，与A挨着的只有3个词，那A的负采样就有997个，但如果只有100个词，与A挨着的只有3个词，那A的负采样就有97个，每个词都有那么多的负采样得呈指数增长。小规模的的每个词的负采样可以适当增加一点，规模越大，负采样的个数尽量越少。概率取3/4是试出来的效果相对好。

![负采样](C:\Users\HP\Desktop\read\面试\NLP面经\pic\负采样.png)



<u>3、hierarchical softmax和负采样的原理和复杂度</u>

Hierarchical Softmax采用的树是二叉树。它将树上的叶子节点分配给词典里的词，同时将从树根到叶子节点的路径上的每个非叶子结点都看作二分类，由此，路径上二分类概率连乘的结果就是该叶子节点对应的词的概率。

一个full softmax需要一次计算所有的W个词，而Hierarchical Softmax却只需要计算大约$log_2(W)$（即，树根到该叶子节点的路径长度）个词，大大减少了计算的复杂度。

实际应用中，Hierarchical Softmax采用是Huffman树而非其他二叉树，这是因为Huffman树对于高频词会赋予更短的编码，使得高频词离根节点距离更近，从而使得训练速度加快。

负采样时间复杂度是kV，k是窗口大小，V是词典大小。



<u>4、哈夫曼树的构建方法，在NLP有啥应用</u>

hierarchical softmax，利用哈夫曼树构造很多二分类(sigmoid)，用以词表中的全部词作为叶子节点，词频作为节点的权，构建Huffman树，作为输出。从根节点出发，到达指定叶子节点的路径。Hierarchical Softmax正是利用这条路径来计算指定词的概率，而非用softmax来计算。 



<u>5、word2vec和glove的区别</u>



<u>6、怎样评估词向量的质量</u>

相似度任务word similarity task 和词汇类比任务word analogy task。但是一般还得根据具体任务进行测评包括句子分类，文本分类，词性标注等。

word similarity目的是评估词向量模型在两个词之间的语义紧密度和相关性的能力，例如男人与女人，男孩与女孩，中国与北京这些词对之间的相似度。一般采用 `斯皮尔曼等级相关系数（ρ）` 简写为 `rho`，先得到cos值，再计算rho

word analogy考察了用词向量来推断不同单词之间的语义关系的能力。在这个任务中，三个单词 a ，b 和 s 被给出，目标是推断出第四个单词 t 满足`a 是 b，t 和 s 是相似的` ，例如，我们要完成一句话，`巴黎到法国像罗马到（）`？巴黎与法国，这两个词之间是有语义关系的（巴黎是法国的首都），那么，根据第三个词（罗马），可以推断出是意大利。我们可以根据矢量的加减来做到这一点，因为这些单词在空间上具有特定的关系。



<u>7、选出当前query和100万个key词向量相似度的TopK，复杂度尽可能低（faiss）</u>



<u>8、fasttext与CBOW的异同</u>

相同：

- 模型架构相似
- 都用了分层softmax进行优化，其中fasttext频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高。

不同：

- fasttext的输入：句子中的每个词和句子的ngram特征，特征向量通过线性变换映射到中间层，中间层再映射到标签。fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。

  cbow的输入：中间词的上下文，与完整句子无关

  **关于ngram**：假设：第n个词出现与前n-1个词相关，而与其他任何词不相关

  cbow不考虑词之间的顺序，即没有ngram，fasttext加入ngram

- fasttext是文本分类模型，是有监督模型；可以完成无监督的词向量学习

  cbow是训练词向量算法，是无监督模型



2、CBOW的过程

- 输入层：上下文单词的onehot.  {假设单词向量空间dim为V，上下文单词个数为C}
- 所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，初始化权重矩阵W}      $W^Tx$ {N*V * V*1}
- 所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N.
- 乘以输出权重矩阵W' {N*V}
- 得到向量 {1*V} 激活函数处理得到V-dim概率分布  {PS: 因为是onehot嘛，其中的每一维斗代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word）
- 与true label的onehot做比较，误差越小越好

3、skip-gram过程

