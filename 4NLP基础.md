## Subword  (将词变为更小单位的算法的总称)

1、BPE获得Subword的步骤如下：GPT/RoBERTa

- 准备足够大的训练语料，并确定期望的Subword词表大小；
- 将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表；
- 在语料上统计单词内相邻单元对的频数，选取**频数**最高的单元对合并成新的Subword单元；
- 重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1.

BERT使用BPE因为BPE能很好地解决OOV，在语义粒度上是比较合适的表达。



2、WordPiece BERT

与BPE算法类似，WordPiece算法也是每次从词表中选出两个子词合并成新的子词。与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择**能够提升语言模型概率最大的相邻子词加入词表。**

3、ULM

与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是**减量法**,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。



## 自回归与自编码

单向特征表示的自回归预训练语言模型，统称为单向模型：ELMO、GPT、ULMFiT、SiATL

双向特征表示的自编码预训练语言模型，统称为BERT系列模型：BERT、RoBERTa、ERNIE1.0/ERNIE(THU)

双向特征表示的自回归预训练语言模型：XLNet：将自回归LM方向引入双向语言模型方面

**自回归语言模型**：

优点：文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程

缺点：只能利用上文或者下文的信息，不能同时利用上文和下文的信息

**自编码语言模型：**

优点：能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文

缺点：主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的

# TF-IDF

TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

上述引用总结就是, **一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。**这也就是TF-IDF的含义。

**TF**(Term Frequency, 词频)表示词条在文本中出现的频率



权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。

**IDF**(Inverse Document Frequency, 逆文件频率)**表示关键词的普遍程度

某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到

# jieba分词

流程：

1. 依据统计词典（模型中这部分已经具备，也可自定义加载）构建统计词典中词的前缀词典。

   前缀词典构造如下，它是将在统计词典中出现的每一个词的每一个前缀提取出来，统计词频，如果某个前缀词在统计词典中没有出现，词频统计为0

2. 依据前缀词典对输入的句子进行DAG（有向无环图）的构造。

   以每个字所在的位置为键值key，相应划分的末尾位置构成的列表为value

3. 使用动态规划的方法在DAG上找到一条概率最大路径，依据此路径进行分词。

   每一个词出现的概率等于该词在前缀里的词频除以所有词的词频之和。如果词频为0或是不存在，当做词频为1来处理。

   这里会取对数概率，即在每个词概率的基础上取对数，一是为了防止下溢，二后面的概率相乘可以变成相加计算。

4. 对于未收录词（是指不在统计词典中出现的词），使用HMM(隐马尔克夫模型)模型，用Viterbi（维特比）算法找出最可能出现的隐状态序列。

